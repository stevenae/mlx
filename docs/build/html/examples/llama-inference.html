<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM inference &mdash; MLX 0.0.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Developer Documentation" href="../dev/extensions.html" />
    <link rel="prev" title="Multi-Layer Perceptron" href="mlp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MLX
          </a>
              <div class="version">
                0.0.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Build and Install</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using_streams.html">Using Streams</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LLM inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#implementing-the-model">Implementing the model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#attention-layer">Attention layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#encoder-layer">Encoder layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-model">Full model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generation">Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#putting-it-all-together">Putting it all together</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#converting-the-weights">Converting the weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="#weight-loading-and-benchmarking">Weight loading and benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scripts">Scripts</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Further Reading</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/extensions.html">Developer Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python/array.html">Array</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/devices_and_streams.html">Devices and Streams</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/ops.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/random.html">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/fft.html">FFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/nn.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/tree_utils.html">Tree Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C++ API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpp/ops.html">Operations</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MLX</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LLM inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/llama-inference.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-inference">
<h1>LLM inference<a class="headerlink" href="#llm-inference" title="Permalink to this heading"></a></h1>
<p>MLX enables efficient inference of large-ish transformers on Apple silicon
without compromising on ease of use. In this example we will create an
inference script for the Llama family of transformer models in which the model
is defined in less than 200 lines of python.</p>
<section id="implementing-the-model">
<h2>Implementing the model<a class="headerlink" href="#implementing-the-model" title="Permalink to this heading"></a></h2>
<p>We will use the neural network building blocks defined in the <code class="xref py py-mod docutils literal notranslate"><span class="pre">mlx.nn</span></code>
module to concisely define the model architecture.</p>
<section id="attention-layer">
<h3>Attention layer<a class="headerlink" href="#attention-layer" title="Permalink to this heading"></a></h3>
<p>We will start with the llama attention layer which notably uses the RoPE
positional encoding. <a class="footnote-reference brackets" href="#id5" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> In addition, our attention layer will optionally use a
key/value cache that will be concatenated with the provided keys and values to
support efficient inference.</p>
<p>Our implementation uses <a class="reference internal" href="../python/_autosummary/mlx.nn.Linear.html#mlx.nn.Linear" title="mlx.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlx.nn.Linear</span></code></a> for all the projections and
<a class="reference internal" href="../python/_autosummary/mlx.nn.RoPE.html#mlx.nn.RoPE" title="mlx.nn.RoPE"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlx.nn.RoPE</span></code></a> for the positional encoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlx.core</span> <span class="k">as</span> <span class="nn">mx</span>
<span class="kn">import</span> <span class="nn">mlx.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rope</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RoPE</span><span class="p">(</span><span class="n">dims</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">traditional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_proj</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_proj</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_proj</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

        <span class="c1"># Extract some shapes</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Prepare the queries, keys and values for the attention computation</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># Add RoPE to the queries and keys and combine them with the cache</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">cache</span>
            <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rope</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">key_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rope</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">key_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">keys</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">key_cache</span><span class="p">,</span> <span class="n">keys</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">value_cache</span><span class="p">,</span> <span class="n">values</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rope</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
            <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rope</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

        <span class="c1"># Finally perform the attention computation</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">queries</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">mask</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">values_hat</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span> <span class="o">@</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Note that we return the keys and values to possibly be used as a cache</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">values_hat</span><span class="p">),</span> <span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="encoder-layer">
<h3>Encoder layer<a class="headerlink" href="#encoder-layer" title="Permalink to this heading"></a></h3>
<p>The other component of the Llama model is the encoder layer which uses RMS
normalization <a class="footnote-reference brackets" href="#id6" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> and SwiGLU. <a class="footnote-reference brackets" href="#id7" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> For RMS normalization we will use
<a class="reference internal" href="../python/_autosummary/mlx.nn.RMSNorm.html#mlx.nn.RMSNorm" title="mlx.nn.RMSNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlx.nn.RMSNorm</span></code></a> that is already provided in <code class="xref py py-mod docutils literal notranslate"><span class="pre">mlx.nn</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LlamaEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">LlamaAttention</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">mx</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</section>
<section id="full-model">
<h3>Full model<a class="headerlink" href="#full-model" title="Permalink to this heading"></a></h3>
<p>To implement any Llama model we simply have to combine <code class="docutils literal notranslate"><span class="pre">LlamaEncoderLayer</span></code>
instances with an <a class="reference internal" href="../python/_autosummary/mlx.nn.Embedding.html#mlx.nn.Embedding" title="mlx.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlx.nn.Embedding</span></code></a> to embed the input tokens.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Llama</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">LlamaEncoderLayer</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="o">.</span><span class="n">create_additive_causal_mask</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that in the implementation above we use a simple list to hold the encoder
layers but using <code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code> will still consider these layers.</p>
</section>
<section id="generation">
<h3>Generation<a class="headerlink" href="#generation" title="Permalink to this heading"></a></h3>
<p>Our <code class="docutils literal notranslate"><span class="pre">Llama</span></code> module can be used for training but not inference as the
<code class="docutils literal notranslate"><span class="pre">__call__</span></code> method above processes one input, completely ignores the cache and
performs no sampling whatsoever. In the rest of this subsection, we will
implement the inference function as a python generator that processes the
prompt and then autoregressively yields tokens one at a time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Llama</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">cache</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Make an additive causal mask. We will need that to process the prompt.</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="o">.</span><span class="n">create_additive_causal_mask</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># First we process the prompt x the same way as in __call__ but</span>
        <span class="c1"># save the caches in cache</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>  <span class="c1"># &lt;--- we store the per layer cache in a</span>
                             <span class="c1">#      simple python list</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># &lt;--- we only care about the last logits</span>
                                     <span class="c1">#      that generate the next token</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">temp</span><span class="p">))</span>

        <span class="c1"># y now has size [1]</span>
        <span class="c1"># Since MLX is lazily evaluated nothing is computed yet.</span>
        <span class="c1"># Calling y.item() would force the computation to happen at</span>
        <span class="c1"># this point but we can also choose not to do that and let the</span>
        <span class="c1"># user choose when to start the computation.</span>
        <span class="k">yield</span> <span class="n">y</span>

        <span class="c1"># Now we parsed the prompt and generated the first token we</span>
        <span class="c1"># need to feed it back into the model and loop to generate the</span>
        <span class="c1"># rest.</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># Unsqueezing the last dimension to add a sequence length</span>
            <span class="c1"># dimension of 1</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cache</span><span class="p">)):</span>
                <span class="c1"># We are overwriting the arrays in the cache list. When</span>
                <span class="c1"># the computation will happen, MLX will be discarding the</span>
                <span class="c1"># old cache the moment it is not needed anymore.</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">temp</span><span class="p">))</span>

            <span class="k">yield</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="putting-it-all-together">
<h3>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this heading"></a></h3>
<p>We now have everything we need to create a Llama model and sample tokens from
it. In the following code, we randomly initialize a small Llama model, process
6 tokens of prompt and generate 10 tokens.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">mlp_dims</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Since MLX is lazily evaluated nothing has actually been materialized yet.</span>
<span class="c1"># We could have set the `dims` to 20_000 on a machine with 8GB of RAM and the</span>
<span class="c1"># code above would still run. Let&#39;s actually materialize the model.</span>
<span class="n">mx</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>  <span class="c1"># &lt;-- Note the double brackets because we</span>
                                            <span class="c1">#     have a batch dimension even</span>
                                            <span class="c1">#     though it is 1 in this case</span>

<span class="n">generated</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">))]</span>

<span class="c1"># Since we haven&#39;t evaluated anything, nothing is computed yet. The list</span>
<span class="c1"># `generated` contains the arrays that hold the computation graph for the</span>
<span class="c1"># full processing of the prompt and the generation of 10 tokens.</span>
<span class="c1">#</span>
<span class="c1"># We can evaluate them one at a time, or all together. Concatenate them or</span>
<span class="c1"># print them. They would all result in very similar runtimes and give exactly</span>
<span class="c1"># the same results.</span>
<span class="n">mx</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">generated</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="converting-the-weights">
<h2>Converting the weights<a class="headerlink" href="#converting-the-weights" title="Permalink to this heading"></a></h2>
<p>This section assumes that you have access to the original Llama weights and the
SentencePiece model that comes with them. We will write a small script to
convert the PyTorch weights to MLX compatible ones and write them in a NPZ file
that can be loaded directly by MLX.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">starmap</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">map_torch_to_mlx</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;tok_embedding&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;embedding.weight&quot;</span>

    <span class="k">elif</span> <span class="s2">&quot;norm&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;attention_norm&quot;</span><span class="p">,</span> <span class="s2">&quot;norm1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ffn_norm&quot;</span><span class="p">,</span> <span class="s2">&quot;norm2&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="s2">&quot;wq&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;wk&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;wv&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;wo&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;wq&quot;</span><span class="p">,</span> <span class="s2">&quot;query_proj&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;wk&quot;</span><span class="p">,</span> <span class="s2">&quot;key_proj&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;wv&quot;</span><span class="p">,</span> <span class="s2">&quot;value_proj&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;wo&quot;</span><span class="p">,</span> <span class="s2">&quot;out_proj&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="s2">&quot;w1&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;w2&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;w3&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="c1"># The FFN is a separate submodule in PyTorch</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;feed_forward.w1&quot;</span><span class="p">,</span> <span class="s2">&quot;linear1&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;feed_forward.w3&quot;</span><span class="p">,</span> <span class="s2">&quot;linear2&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;feed_forward.w2&quot;</span><span class="p">,</span> <span class="s2">&quot;linear3&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="s2">&quot;output&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;out_proj&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="s2">&quot;rope&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Convert Llama weights to MLX&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;torch_weights&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;output_file&quot;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">torch_weights</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">output_file</span><span class="p">,</span>
        <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">starmap</span><span class="p">(</span><span class="n">map_torch_to_mlx</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="weight-loading-and-benchmarking">
<h2>Weight loading and benchmarking<a class="headerlink" href="#weight-loading-and-benchmarking" title="Permalink to this heading"></a></h2>
<p>After converting the weights to be compatible to our implementation, all that is
left is to load them from disk and we can finally use the LLM to generate text.
We can load numpy format files using the <a class="reference internal" href="../python/_autosummary/mlx.core.load.html#mlx.core.load" title="mlx.core.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlx.core.load()</span></code></a> operation.</p>
<p>To create a parameter dictionary from the key/value representation of NPZ files
we will use the <a class="reference internal" href="../python/_autosummary/mlx.utils.tree_unflatten.html#mlx.utils.tree_unflatten" title="mlx.utils.tree_unflatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlx.utils.tree_unflatten()</span></code></a> helper method as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlx.utils</span> <span class="kn">import</span> <span class="n">tree_unflatten</span>

<span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weight_file</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">())))</span>
</pre></div>
</div>
<p><a class="reference internal" href="../python/_autosummary/mlx.utils.tree_unflatten.html#mlx.utils.tree_unflatten" title="mlx.utils.tree_unflatten"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mlx.utils.tree_unflatten()</span></code></a> will take keys from the NPZ file that look
like <code class="docutils literal notranslate"><span class="pre">layers.2.attention.query_proj.weight</span></code> and will transform them to</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;query_proj&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">}}}]}</span>
</pre></div>
</div>
<p>which can then be used to update the model. Note that the method above incurs
several unnecessary copies from disk to numpy and then from numpy to MLX. It
will be replaced in the future with direct loading to MLX.</p>
<p>You can download the full example code in <a class="reference external" href="code">mlx-examples</a>. Assuming, the
existence of <code class="docutils literal notranslate"><span class="pre">weights.pth</span></code> and <code class="docutils literal notranslate"><span class="pre">tokenizer.model</span></code> in the current working
directory we can play around with our inference script as follows (the timings
are representative of an M1 Ultra and the 7B parameter Llama model):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>convert.py<span class="w"> </span>weights.pth<span class="w"> </span>llama-7B.mlx.npz
$<span class="w"> </span>python<span class="w"> </span>llama.py<span class="w"> </span>llama-7B.mlx.npz<span class="w"> </span>tokenizer.model<span class="w"> </span><span class="s1">&#39;Call me Ishmael. Some years ago never mind how long precisely&#39;</span>
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Loading<span class="w"> </span>model<span class="w"> </span>from<span class="w"> </span>disk:<span class="w"> </span><span class="m">5</span>.247<span class="w"> </span>s
Press<span class="w"> </span>enter<span class="w"> </span>to<span class="w"> </span>start<span class="w"> </span>generation
------
,<span class="w"> </span>having<span class="w"> </span>little<span class="w"> </span>or<span class="w"> </span>no<span class="w"> </span>money<span class="w"> </span><span class="k">in</span><span class="w"> </span>my<span class="w"> </span>purse,<span class="w"> </span>and<span class="w"> </span>nothing<span class="w"> </span>of<span class="w"> </span>greater<span class="w"> </span>consequence<span class="w"> </span><span class="k">in</span><span class="w"> </span>my<span class="w"> </span>mind,<span class="w"> </span>I<span class="w"> </span>happened<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>walking<span class="w"> </span>down<span class="w"> </span>Gower<span class="w"> </span>Street<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>afternoon,<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>heavy<span class="w"> </span>rain,<span class="w"> </span>and<span class="w"> </span>I<span class="w"> </span>saw<span class="w"> </span>a<span class="w"> </span>few<span class="w"> </span>steps<span class="w"> </span>off,<span class="w"> </span>a<span class="w"> </span>man<span class="w"> </span><span class="k">in</span><span class="w"> </span>rags,<span class="w"> </span>who<span class="w"> </span>sat<span class="w"> </span>upon<span class="w"> </span>his<span class="w"> </span>bundle<span class="w"> </span>and<span class="w"> </span>looked<span class="w"> </span>hard<span class="w"> </span>into<span class="w"> </span>the<span class="w"> </span>wet<span class="w"> </span>as<span class="w"> </span><span class="k">if</span><span class="w"> </span>he<span class="w"> </span>were<span class="w"> </span>going<span class="w"> </span>to<span class="w"> </span>cry.<span class="w"> </span>I<span class="w"> </span>watched<span class="w"> </span>him<span class="w"> </span>attentively<span class="w"> </span><span class="k">for</span><span class="w"> </span>some<span class="w"> </span>time,<span class="w"> </span>and<span class="w"> </span>could<span class="w"> </span>not<span class="w"> </span>but<span class="w"> </span>observe<span class="w"> </span>that,<span class="w"> </span>though<span class="w"> </span>a<span class="w"> </span>numerous<span class="w"> </span>crowd<span class="w"> </span>was<span class="w"> </span>hurrying<span class="w"> </span>up<span class="w"> </span>and<span class="w"> </span>down,
------
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Prompt<span class="w"> </span>processing:<span class="w"> </span><span class="m">0</span>.437<span class="w"> </span>s
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Full<span class="w"> </span>generation:<span class="w"> </span><span class="m">4</span>.330<span class="w"> </span>s
</pre></div>
</div>
<p>We observe that 4.3 seconds are required to generate 100 tokens and 0.4 seconds
of those are spent processing the prompt. This amounts to a little over <strong>39 ms
per token</strong>.</p>
<p>By running with a much bigger prompt we can see that the per token generation
time as well as the prompt processing time remains almost constant.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>llama.py<span class="w"> </span>llama-7B.mlx.npz<span class="w"> </span>tokenizer.model<span class="w"> </span><span class="s1">&#39;Call me Ishmael. Some years ago never mind how long precisely, having little or no money in my purse, and nothing of greater consequence in my mind, I happened to be walking down Gower Street in the afternoon, in the heavy rain, and I saw a few steps off, a man in rags, who sat upon his bundle and looked hard into the wet as if he were going to cry. I watched him attentively for some time, and could not but observe that, though a numerous crowd was hurrying up and down, nobody took the least notice of him. I stopped at last, at a little distance, as if I had been in doubt, and after looking on a few minutes, walked straight up to him. He slowly raised his eyes, and fixed them upon me for a moment, without speaking, and then resumed his place and posture as before. I stood looking at him for a while, feeling very much pain at heart, and then said to him, “What are you doing there?” Something like a smile passed over his face, as he said slowly, “I am waiting for someone; but it has been three quarters of an hour now, and he has not come.” “What is it you are waiting for?” said I. Still he made no immediate reply, but again put his face down upon his hands, and did not&#39;</span>
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Loading<span class="w"> </span>model<span class="w"> </span>from<span class="w"> </span>disk:<span class="w"> </span><span class="m">5</span>.247<span class="w"> </span>s
Press<span class="w"> </span>enter<span class="w"> </span>to<span class="w"> </span>start<span class="w"> </span>generation
------
take<span class="w"> </span>his<span class="w"> </span>eyes<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>ground.<span class="w"> </span>“What<span class="w"> </span>is<span class="w"> </span>it<span class="w"> </span>you<span class="w"> </span>are<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span>?”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“I<span class="w"> </span>am<span class="w"> </span>not<span class="w"> </span>accustomed<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>thus<span class="w"> </span>questioned,”<span class="w"> </span>said<span class="w"> </span>he.<span class="w"> </span>“You<span class="w"> </span>look<span class="w"> </span>like<span class="w"> </span>a<span class="w"> </span>reasonable<span class="w"> </span>man—tell<span class="w"> </span>me,<span class="w"> </span><span class="k">then</span>,<span class="w"> </span>what<span class="w"> </span>are<span class="w"> </span>you<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span>?”<span class="w"> </span>“You<span class="w"> </span>would<span class="w"> </span>not<span class="w"> </span>understand,”<span class="w"> </span>he<span class="w"> </span>replied<span class="p">;</span><span class="w"> </span>“and<span class="w"> </span>how<span class="w"> </span>could<span class="w"> </span>you<span class="w"> </span><span class="nb">help</span><span class="w"> </span>me,<span class="w"> </span><span class="k">if</span><span class="w"> </span>I<span class="w"> </span>were<span class="w"> </span>to<span class="w"> </span>tell<span class="w"> </span>you?”<span class="w"> </span>“I<span class="w"> </span>should<span class="w"> </span>not<span class="w"> </span>only<span class="w"> </span>understand,<span class="w"> </span>but<span class="w"> </span>would<span class="w"> </span><span class="k">do</span><span class="w"> </span>all<span class="w"> </span>that<span class="w"> </span>I<span class="w"> </span>could,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>He<span class="w"> </span>did<span class="w"> </span>not
------
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Prompt<span class="w"> </span>processing:<span class="w"> </span><span class="m">0</span>.579<span class="w"> </span>s
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Full<span class="w"> </span>generation:<span class="w"> </span><span class="m">4</span>.690<span class="w"> </span>s
$<span class="w"> </span>python<span class="w"> </span>llama.py<span class="w"> </span>--num-tokens<span class="w"> </span><span class="m">500</span><span class="w"> </span>llama-7B.mlx.npz<span class="w"> </span>tokenizer.model<span class="w"> </span><span class="s1">&#39;Call me Ishmael. Some years ago never mind how long precisely, having little or no money in my purse, and nothing of greater consequence in my mind, I happened to be walking down Gower Street in the afternoon, in the heavy rain, and I saw a few steps off, a man in rags, who sat upon his bundle and looked hard into the wet as if he were going to cry. I watched him attentively for some time, and could not but observe that, though a numerous crowd was hurrying up and down, nobody took the least notice of him. I stopped at last, at a little distance, as if I had been in doubt, and after looking on a few minutes, walked straight up to him. He slowly raised his eyes, and fixed them upon me for a moment, without speaking, and then resumed his place and posture as before. I stood looking at him for a while, feeling very much pain at heart, and then said to him, “What are you doing there?” Something like a smile passed over his face, as he said slowly, “I am waiting for someone; but it has been three quarters of an hour now, and he has not come.” “What is it you are waiting for?” said I. Still he made no immediate reply, but again put his face down upon his hands, and did not&#39;</span>
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Loading<span class="w"> </span>model<span class="w"> </span>from<span class="w"> </span>disk:<span class="w"> </span><span class="m">5</span>.628<span class="w"> </span>s
Press<span class="w"> </span>enter<span class="w"> </span>to<span class="w"> </span>start<span class="w"> </span>generation
------
take<span class="w"> </span>his<span class="w"> </span>eyes<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>ground.<span class="w"> </span>“What<span class="w"> </span>is<span class="w"> </span>it<span class="w"> </span>you<span class="w"> </span>are<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span>?”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“I<span class="w"> </span>am<span class="w"> </span>not<span class="w"> </span>accustomed<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>thus<span class="w"> </span>questioned,”<span class="w"> </span>said<span class="w"> </span>he.<span class="w"> </span>“You<span class="w"> </span>look<span class="w"> </span>like<span class="w"> </span>a<span class="w"> </span>reasonable<span class="w"> </span>man—tell<span class="w"> </span>me,<span class="w"> </span><span class="k">then</span>,<span class="w"> </span>what<span class="w"> </span>are<span class="w"> </span>you<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span>?”<span class="w"> </span>“You<span class="w"> </span>would<span class="w"> </span>not<span class="w"> </span>understand,”<span class="w"> </span>he<span class="w"> </span>replied<span class="p">;</span><span class="w"> </span>“and<span class="w"> </span>how<span class="w"> </span>could<span class="w"> </span>you<span class="w"> </span><span class="nb">help</span><span class="w"> </span>me,<span class="w"> </span><span class="k">if</span><span class="w"> </span>I<span class="w"> </span>were<span class="w"> </span>to<span class="w"> </span>tell<span class="w"> </span>you?”<span class="w"> </span>“I<span class="w"> </span>should<span class="w"> </span>not<span class="w"> </span>only<span class="w"> </span>understand,<span class="w"> </span>but<span class="w"> </span>would<span class="w"> </span><span class="k">do</span><span class="w"> </span>all<span class="w"> </span>that<span class="w"> </span>I<span class="w"> </span>could,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>He<span class="w"> </span>did<span class="w"> </span>not<span class="w"> </span>reply,<span class="w"> </span>but<span class="w"> </span>still<span class="w"> </span>went<span class="w"> </span>on<span class="w"> </span>looking<span class="w"> </span>at<span class="w"> </span>the<span class="w"> </span>ground,<span class="w"> </span>and<span class="w"> </span>took<span class="w"> </span>hold<span class="w"> </span>of<span class="w"> </span>his<span class="w"> </span>bundle<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>nervous<span class="w"> </span>trembling.<span class="w"> </span>I<span class="w"> </span>waited<span class="w"> </span>some<span class="w"> </span>time,<span class="w"> </span>and<span class="w"> </span><span class="k">then</span><span class="w"> </span>resumed.<span class="w"> </span>“It<span class="w"> </span>is<span class="w"> </span>of<span class="w"> </span>no<span class="w"> </span>use<span class="w"> </span>to<span class="w"> </span>say<span class="w"> </span>you<span class="w"> </span>would<span class="w"> </span>not<span class="w"> </span>understand,<span class="w"> </span><span class="k">if</span><span class="w"> </span>I<span class="w"> </span>were<span class="w"> </span>to<span class="w"> </span>tell<span class="w"> </span>you,”<span class="w"> </span>said<span class="w"> </span>he.<span class="w"> </span>“I<span class="w"> </span>have<span class="w"> </span>not<span class="w"> </span>told<span class="w"> </span>you<span class="w"> </span>why<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>him,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“And<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>sure<span class="w"> </span>I<span class="w"> </span>should<span class="w"> </span>not<span class="w"> </span>understand,”<span class="w"> </span>replied<span class="w"> </span>he.<span class="w"> </span>“I<span class="w"> </span>will<span class="w"> </span>tell<span class="w"> </span>you<span class="w"> </span><span class="k">then</span>,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“and,<span class="w"> </span>perhaps,<span class="w"> </span>you<span class="w"> </span>would<span class="w"> </span>not<span class="w"> </span>be<span class="w"> </span>surprised.”<span class="w"> </span>“No<span class="w"> </span>matter,”<span class="w"> </span>said<span class="w"> </span>he,<span class="w"> </span>“I<span class="w"> </span>shall<span class="w"> </span>be<span class="w"> </span>surprised<span class="w"> </span>anyhow<span class="p">;</span><span class="w"> </span>so<span class="w"> </span>tell<span class="w"> </span>me<span class="w"> </span>why<span class="w"> </span>you<span class="w"> </span>are<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>him.”<span class="w"> </span>“He<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>friend,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“Yes,”<span class="w"> </span>said<span class="w"> </span>he,<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>slight<span class="w"> </span>smile,<span class="w"> </span>“I<span class="w"> </span>know.”<span class="w"> </span>“He<span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>kind<span class="w"> </span>to<span class="w"> </span>me,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“and<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>him.<span class="w"> </span>I<span class="w"> </span>want<span class="w"> </span>to<span class="w"> </span>see<span class="w"> </span>him,<span class="w"> </span>and<span class="w"> </span>could<span class="w"> </span>have<span class="w"> </span>waited<span class="w"> </span>as<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>now,<span class="w"> </span><span class="k">for</span><span class="w"> </span>a<span class="w"> </span>much<span class="w"> </span>longer<span class="w"> </span>time.”<span class="w"> </span>“He<span class="w"> </span>will<span class="w"> </span>not<span class="w"> </span>soon<span class="w"> </span>come,”<span class="w"> </span>said<span class="w"> </span>he.<span class="w"> </span>“Unless<span class="w"> </span>he<span class="w"> </span>sees<span class="w"> </span>you<span class="w"> </span>here,<span class="w"> </span>he<span class="w"> </span>will<span class="w"> </span>not<span class="w"> </span>know<span class="w"> </span>of<span class="w"> </span>your<span class="w"> </span>having<span class="w"> </span>waited,<span class="w"> </span>and<span class="w"> </span>he<span class="w"> </span>will<span class="w"> </span>be<span class="w"> </span>very<span class="w"> </span>unlikely<span class="w"> </span>to<span class="w"> </span>come.”<span class="w"> </span>“No<span class="w"> </span>matter,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“I<span class="w"> </span>shall<span class="w"> </span><span class="nb">wait</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>him.”<span class="w"> </span>“This<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>strange<span class="w"> </span>thing,”<span class="w"> </span>said<span class="w"> </span>he,<span class="w"> </span>still<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>same<span class="w"> </span>amused<span class="w"> </span>smile.<span class="w"> </span>“How<span class="w"> </span>did<span class="w"> </span>you<span class="w"> </span>know,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“that<span class="w"> </span>he<span class="w"> </span>was<span class="w"> </span>coming?<span class="w"> </span>How<span class="w"> </span>should<span class="w"> </span>you<span class="w"> </span>be<span class="w"> </span>waiting?”<span class="w"> </span>“That<span class="w"> </span>is<span class="w"> </span>my<span class="w"> </span>secret,”<span class="w"> </span>said<span class="w"> </span>he.<span class="w"> </span>“And<span class="w"> </span>you<span class="w"> </span>expect<span class="w"> </span>him?”<span class="w"> </span>“Yes,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“Are<span class="w"> </span>you<span class="w"> </span>disappointed<span class="w"> </span><span class="k">then</span>,<span class="w"> </span><span class="k">if</span><span class="w"> </span>he<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>come?”<span class="w"> </span>“No,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“it<span class="w"> </span>is<span class="w"> </span>his<span class="w"> </span>secret,<span class="w"> </span>not<span class="w"> </span>mine.”<span class="w"> </span>“If<span class="w"> </span>he<span class="w"> </span>comes,”<span class="w"> </span>said<span class="w"> </span>he,<span class="w"> </span>“do<span class="w"> </span>you<span class="w"> </span>mean<span class="w"> </span>to<span class="w"> </span>go<span class="w"> </span>straight<span class="w"> </span>away?”<span class="w"> </span>“Yes,”<span class="w"> </span>said<span class="w"> </span>I,<span class="w"> </span>“I<span class="w"> </span>cannot<span class="w"> </span>be<span class="w"> </span>happy<span class="w"> </span><span class="k">if</span><span class="w"> </span>I<span class="w"> </span><span class="k">do</span><span class="w"> </span>not<span class="w"> </span>go<span class="w"> </span>straight<span class="w"> </span>away<span class="w"> </span>after<span class="w"> </span>him.”<span class="w"> </span>“Did<span class="w"> </span>you<span class="w"> </span>know<span class="w"> </span>this<span class="w"> </span>place<span class="w"> </span>before?”<span class="w"> </span>asked<span class="w"> </span>he.<span class="w"> </span>“Yes,”<span class="w"> </span>said<span class="w"> </span>I.<span class="w"> </span>“Is<span class="w"> </span>there<span class="w"> </span>any<span class="w"> </span>shop<span class="w"> </span>to<span class="w"> </span>buy<span class="w"> </span>food<span class="w"> </span>here?”<span class="w"> </span>“
------
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Prompt<span class="w"> </span>processing:<span class="w"> </span><span class="m">0</span>.633<span class="w"> </span>s
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span>Full<span class="w"> </span>generation:<span class="w"> </span><span class="m">21</span>.475<span class="w"> </span>s
</pre></div>
</div>
</section>
<section id="scripts">
<h2>Scripts<a class="headerlink" href="#scripts" title="Permalink to this heading"></a></h2>
<div class="admonition-download-the-code admonition">
<p class="admonition-title">Download the code</p>
<p>The full example code is available in <a class="reference external" href="code">mlx-examples</a>.</p>
</div>
<aside class="footnote brackets" id="id5" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B. and Liu, Y., 2021.
Roformer: Enhanced transformer with rotary position embedding. arXiv
preprint arXiv:2104.09864.</p>
</aside>
<aside class="footnote brackets" id="id6" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Zhang, B. and Sennrich, R., 2019. Root mean square layer normalization.
Advances in Neural Information Processing Systems, 32.</p>
</aside>
<aside class="footnote brackets" id="id7" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Shazeer, N., 2020. Glu variants improve transformer. arXiv preprint
arXiv:2002.05202.</p>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mlp.html" class="btn btn-neutral float-left" title="Multi-Layer Perceptron" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../dev/extensions.html" class="btn btn-neutral float-right" title="Developer Documentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, MLX Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>